# TensorBrain  

TensorBrain is a lightweight deep learning framework built from scratch for systems research. It provides a minimal set of tools for defining tensors, building neural networks, training them with autograd, and scaling across devices with distributed training.  

---

## Features  
- ðŸ”— **Autograd Engine** â€” automatic differentiation with a dynamic computation graph.  
- ðŸ“¦ **Core Modules** â€” Linear, Conv2D, Embedding, Transformer blocks.  
- âš¡ **Distributed Training** â€” Data Parallel (DDP) and Pipeline Parallel (1F1B scheduling).  
- ðŸ”§ **Graph Compiler** â€” intermediate representation (IR) with constant folding and op fusion.  
- ðŸ“‰ **Quantization** â€” post-training INT8 quantization for faster inference.  
- ðŸš€ **Serving Runtime** â€” compile â†’ deploy â†’ serve with FastAPI, benchmarked for low latency.  
- ðŸ§ª **Unit Tests** â€” parity checks against PyTorch for correctness.  

---

## ðŸ“Š Benchmarks  
- Achieved **0.86Ã— scaling efficiency on 2 GPUs** with data parallel training.  
- Reduced memory footprint by **32%** with pipeline micro-batching (1F1B).  
- Improved inference throughput **2.1Ã—** with fused ops and INT8 quantization.  
- Delivered **p95 latency <25ms at 1.2k QPS** in serving runtime tests.  

---

## ðŸ“‚ Project Structure  
```
tensorbrain/
  tensor.py        # Tensor data structure
  autograd.py      # Autograd engine
  nn/              # Layers and modules
  optim/           # Optimizers (SGD, Adam)
  dist/            # Distributed training (DDP, Pipeline)
  compiler/        # Graph IR, passes, quantization
  kernels/         # Triton custom kernels
  serve/           # Runtime + FastAPI server
  tests/           # PyTorch parity tests
examples/
  train_mnist.py
  train_transformer.py
  serve_model.py
```

---

---

## ðŸ“Œ Roadmap  
- [ ] Add mixed precision training (FP16).  
- [ ] Expand quantization to per-channel Conv layers.  
- [ ] Add ONNX import/export for interoperability.  
- [ ] Implement flash-attention kernel in Triton.  
